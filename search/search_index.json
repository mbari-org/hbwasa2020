{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Note: This site is a markdown-based version of https://doi.org/10.1121/1.5147215 as presented at the ASA Acoustics Virtually Everywhere conference on Dec 9, 2020. Humpback Whale Song Analysis Based on Automatic Classification Performance Carlos A. Rueda and John P. Ryan MBARI \u2013 Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA Abstract \u00b6 Song is a prevalent behavior in humpback whale populations, even in regions that are considered to be foraging habitat such as the Monterey Bay National Marine Sanctuary in the Northeast Pacific, where song is detected nine months out of the year. In this work we explore various machine learning methods to classify song units, as a basis for studying song structure and its changes. We report on a number of analyses and classification exercises based on linear predictive coding, vector quantization, and machine learning classifiers including Naive Bayes, first-order Markov chain, and Hidden Markov modeling. As a baseline for comparison purposes, the distortion measure used to create the codebooks for vector quantization is itself also used as a means for classification. With classification accuracy ranging from 88% to 94% across the selected methods on a 4.5 hour recording involving 4539 unit occurrences over 8 different unit types, we evaluate the effect of several signal processing, clustering, and learning parameters on classification performance with the goal of laying a foundation that can be used to characterize song vocalization at not only the unit level but also below (subunit) and above (phrase, theme, song).","title":"Abstract"},{"location":"#abstract","text":"Song is a prevalent behavior in humpback whale populations, even in regions that are considered to be foraging habitat such as the Monterey Bay National Marine Sanctuary in the Northeast Pacific, where song is detected nine months out of the year. In this work we explore various machine learning methods to classify song units, as a basis for studying song structure and its changes. We report on a number of analyses and classification exercises based on linear predictive coding, vector quantization, and machine learning classifiers including Naive Bayes, first-order Markov chain, and Hidden Markov modeling. As a baseline for comparison purposes, the distortion measure used to create the codebooks for vector quantization is itself also used as a means for classification. With classification accuracy ranging from 88% to 94% across the selected methods on a 4.5 hour recording involving 4539 unit occurrences over 8 different unit types, we evaluate the effect of several signal processing, clustering, and learning parameters on classification performance with the goal of laying a foundation that can be used to characterize song vocalization at not only the unit level but also below (subunit) and above (phrase, theme, song).","title":"Abstract"},{"location":"classification/","text":"Classification \u00b6 VQ Based Classification \u00b6 Along with the probabilistic methods (see below), and as a means to further evaluate the clustering/VQ mechanisms, we also include a classification method based on the quantization distortion measure. VQ Learning : A codebook \\(C^{(u)}\\) is generated for each class \\(u\\) using the designated training instances \\(\\{a\\}\\) for that class. VQ Classification : Classification of a particular unit instance \\(a = \\langle a_1, \\ldots, a_T \\rangle\\) is given by the trained codebook that gets the minimum average distortion in the quantization of \\(a\\) : \\[ u^* = \\operatorname{argmin}\\limits_{u=1}^U d(a, C^{(u)}) \\] where \\(d(a, C^{(u)})\\) is defined as the average quantization distortion over the vectors in \\(a\\) when using \\(C^{(u)}\\) . Probabilistic methods In each of the methods below, by using the training data in a supervised manner, we generate a set of models \\(H = \\{h_1, \\ldots, h_R\\}\\) , one model for each class such that the model \\(h_r\\) learns to recognize \\(u_r\\) instances in a probabilistic sense. Subsequently, given a song unit instance \\(o\\) , we find its most probable \\(h* \\in H\\) , that is, \\(h* = \\operatorname{argmax}_h P[h|o]\\) , as the rule for classification [5, 6]. By using Bayes' formula and assuming \\(P[h]\\) equiprobable -in other words, by using a maximum likelihood criterium- the classification of \\(o\\) is then given by \\(h* = \\operatorname{argmax}_h P[o|h]\\) . Note : In these methods, the underlying codebooks for quantization are those generated with all training song unit instances regardless of class. Naive Bayes \u00b6 The Naive Bayes (NB) classifier is a simple method shown to be quite effective in many practical applications [5]. Its core underlying assumption is that the observed symbols for a class are independent from each other. A Naive Bayes model is defined by: \\(N\\) : Number of attributes (observable symbols) \\(\\pi\\) : Observation probability distribution Example of a model with 3 observable symbols \\(\\{s_1, s_2, s_3\\}\\) and an associated observed sequence: 1 NB Learning : A model for class \\(u\\) is generated with the designated training observation sequences using an \\(m\\) -estimate (pseudocounts) to determine the \\(\\pi\\) distribution [5]. Each element of the distribution is determined as: \\[ \\pi_m^{(u)} \\gets \\frac{n_m + 1}{n + M} \\] where \\(n_m\\) is the number of quantized symbols equal to \\(m\\) and \\(n\\) is the total number of symbols across all training sequence from class \\(u\\) . NB Classification : The probability of a sequence \\(o\\) given \\(a\\) model is: \\[ P[o | \\pi^{(u)}] = \\prod\\limits_{t=1}^T P[o_t\\ |\\ \\pi^{(u)}] = \\prod\\limits_{t=1}^T \\pi_{o_t}^{(u)} \\] With this, as indicated above, we use a maximum likelihood criterium for classification: \\[ u^* = \\operatorname{argmax}_u \\prod\\limits_{t=1}^T \\pi_{o_t}^{(u)} \\] Markov Chain \u00b6 A first-order Markov chain model (MM) allows us to start incorporating a time dependency in the observed sequences [6]. In this model, for a given sequence, each observed symbol corresponds to a state of the system, and the probability of observing a particular symbol at time \\(t\\) only depends on the symbol observed at time \\(t - 1\\) . A Markov Chain model is defined by: \\(N\\) : Number of states (observable symbols in our case) \\(\\pi\\) : Initial state probability distribution \\(a\\) : State probability distributions Example of a model with 2 observable symbols \\(\\{s_1, s_2\\}\\) and an associated observed sequence: MM Learning : Basically done as with NB learning, here we also use pseudocounts to determine the \\(\\pi\\) and \\(a\\) distributions based on the given training sequences. MM Classification : With \\(\\mathcal{M} = (\\pi, a)\\) denoting a Markov chain model, the probability of a sequence \\(o\\) given \\(\\mathcal{M}\\) is: \\[ P[o|\\mathcal{M}] = \\pi_{o_1} \\prod\\limits_{t=1}^{T-1} a_{o_t, o_{t+1}} \\] which we use for our maximum likelihoodclassification: \\[ u^* = \\operatorname{argmax}_u P[o | \\mathcal{M}^{(u)}] \\] Hidden Markov Modeling \u00b6 In HMM, states no longer represent the observable elements of the underlying stochastic process as in the Markov chain case. They are now hidden random variables on which output symbols are defined as probabilistic functions [7-9]. In this study we use discrete observation distributions. An HMM model is defined by: \\(N\\) , \\(M\\) : Number of states and symbols \\(\\pi\\) : Initial state probability distribution \\(a\\) : State probability distributions \\(b\\) : Observation symbol distributions Example of a model with \\(N = 2\\) states \\(\\{s_1, s_2\\}\\) and \\(M = 3\\) symbols \\(\\{v_1, v_2, v_3\\}\\) , and an associated observed sequence: HMM Learning : Implemented via the Baum-Welch algorithm [7]. HMM Classification : With \\(\\lambda = (\\pi, a, b)\\) denoting an HMM, the key operation is to compute \\(P[o|\\lambda]\\) , the probablity of a sequence given \\(\\lambda\\) . For this we use the forward-backward algorithm [7]. Once again, we use a maximum likelihood decision rule: \\[ u^* = \\operatorname{argmax}_u P[o | \\lambda^{(u)}] \\] Arrow notation used here and in the following: dashed lines originating from a node represent probabilities defining a particular distribution; and filled lines represent the ocurrence of a particular event according to the distribution indicated in the label. \u21a9","title":"Classification"},{"location":"classification/#classification","text":"","title":"Classification"},{"location":"classification/#vq-based-classification","text":"Along with the probabilistic methods (see below), and as a means to further evaluate the clustering/VQ mechanisms, we also include a classification method based on the quantization distortion measure. VQ Learning : A codebook \\(C^{(u)}\\) is generated for each class \\(u\\) using the designated training instances \\(\\{a\\}\\) for that class. VQ Classification : Classification of a particular unit instance \\(a = \\langle a_1, \\ldots, a_T \\rangle\\) is given by the trained codebook that gets the minimum average distortion in the quantization of \\(a\\) : \\[ u^* = \\operatorname{argmin}\\limits_{u=1}^U d(a, C^{(u)}) \\] where \\(d(a, C^{(u)})\\) is defined as the average quantization distortion over the vectors in \\(a\\) when using \\(C^{(u)}\\) . Probabilistic methods In each of the methods below, by using the training data in a supervised manner, we generate a set of models \\(H = \\{h_1, \\ldots, h_R\\}\\) , one model for each class such that the model \\(h_r\\) learns to recognize \\(u_r\\) instances in a probabilistic sense. Subsequently, given a song unit instance \\(o\\) , we find its most probable \\(h* \\in H\\) , that is, \\(h* = \\operatorname{argmax}_h P[h|o]\\) , as the rule for classification [5, 6]. By using Bayes' formula and assuming \\(P[h]\\) equiprobable -in other words, by using a maximum likelihood criterium- the classification of \\(o\\) is then given by \\(h* = \\operatorname{argmax}_h P[o|h]\\) . Note : In these methods, the underlying codebooks for quantization are those generated with all training song unit instances regardless of class.","title":"VQ Based Classification"},{"location":"classification/#naive-bayes","text":"The Naive Bayes (NB) classifier is a simple method shown to be quite effective in many practical applications [5]. Its core underlying assumption is that the observed symbols for a class are independent from each other. A Naive Bayes model is defined by: \\(N\\) : Number of attributes (observable symbols) \\(\\pi\\) : Observation probability distribution Example of a model with 3 observable symbols \\(\\{s_1, s_2, s_3\\}\\) and an associated observed sequence: 1 NB Learning : A model for class \\(u\\) is generated with the designated training observation sequences using an \\(m\\) -estimate (pseudocounts) to determine the \\(\\pi\\) distribution [5]. Each element of the distribution is determined as: \\[ \\pi_m^{(u)} \\gets \\frac{n_m + 1}{n + M} \\] where \\(n_m\\) is the number of quantized symbols equal to \\(m\\) and \\(n\\) is the total number of symbols across all training sequence from class \\(u\\) . NB Classification : The probability of a sequence \\(o\\) given \\(a\\) model is: \\[ P[o | \\pi^{(u)}] = \\prod\\limits_{t=1}^T P[o_t\\ |\\ \\pi^{(u)}] = \\prod\\limits_{t=1}^T \\pi_{o_t}^{(u)} \\] With this, as indicated above, we use a maximum likelihood criterium for classification: \\[ u^* = \\operatorname{argmax}_u \\prod\\limits_{t=1}^T \\pi_{o_t}^{(u)} \\]","title":"Naive Bayes"},{"location":"classification/#markov-chain","text":"A first-order Markov chain model (MM) allows us to start incorporating a time dependency in the observed sequences [6]. In this model, for a given sequence, each observed symbol corresponds to a state of the system, and the probability of observing a particular symbol at time \\(t\\) only depends on the symbol observed at time \\(t - 1\\) . A Markov Chain model is defined by: \\(N\\) : Number of states (observable symbols in our case) \\(\\pi\\) : Initial state probability distribution \\(a\\) : State probability distributions Example of a model with 2 observable symbols \\(\\{s_1, s_2\\}\\) and an associated observed sequence: MM Learning : Basically done as with NB learning, here we also use pseudocounts to determine the \\(\\pi\\) and \\(a\\) distributions based on the given training sequences. MM Classification : With \\(\\mathcal{M} = (\\pi, a)\\) denoting a Markov chain model, the probability of a sequence \\(o\\) given \\(\\mathcal{M}\\) is: \\[ P[o|\\mathcal{M}] = \\pi_{o_1} \\prod\\limits_{t=1}^{T-1} a_{o_t, o_{t+1}} \\] which we use for our maximum likelihoodclassification: \\[ u^* = \\operatorname{argmax}_u P[o | \\mathcal{M}^{(u)}] \\]","title":"Markov Chain"},{"location":"classification/#hidden-markov-modeling","text":"In HMM, states no longer represent the observable elements of the underlying stochastic process as in the Markov chain case. They are now hidden random variables on which output symbols are defined as probabilistic functions [7-9]. In this study we use discrete observation distributions. An HMM model is defined by: \\(N\\) , \\(M\\) : Number of states and symbols \\(\\pi\\) : Initial state probability distribution \\(a\\) : State probability distributions \\(b\\) : Observation symbol distributions Example of a model with \\(N = 2\\) states \\(\\{s_1, s_2\\}\\) and \\(M = 3\\) symbols \\(\\{v_1, v_2, v_3\\}\\) , and an associated observed sequence: HMM Learning : Implemented via the Baum-Welch algorithm [7]. HMM Classification : With \\(\\lambda = (\\pi, a, b)\\) denoting an HMM, the key operation is to compute \\(P[o|\\lambda]\\) , the probablity of a sequence given \\(\\lambda\\) . For this we use the forward-backward algorithm [7]. Once again, we use a maximum likelihood decision rule: \\[ u^* = \\operatorname{argmax}_u P[o | \\lambda^{(u)}] \\] Arrow notation used here and in the following: dashed lines originating from a node represent probabilities defining a particular distribution; and filled lines represent the ocurrence of a particular event according to the distribution indicated in the label. \u21a9","title":"Hidden Markov Modeling"},{"location":"conclusions/","text":"Summary and Conclusions \u00b6 Based on classification performance, we empirically determined some potentially good values for signal processing parameters: LPC order: 80-120; Window analysis size: 45-120ms; Window offset: 15ms. The resulting levels of classification accuracy, ranging between 88% and 94%, are encouraging. The relative accuracy of different methods was consistent regardless of parameterization. The VQ classifier allowed us to validate the LPC analysis and clustering phases of our pipeline, while providing a baseline for comparison with the probabilistic models. The Naive Bayes classifier showed great performance despite its simplicity and strong underlying assumption. Although being the least performant, the Markov Chain classifier allowed us to incorporate time dependency as well as a baseline to compare results against other methods, in particular the HMM. The HMM model consistently provided the best performance among the probabilistic models, though just slightly better than Naive Bayes. This is likely due to overfitting, especially given the small size of the dataset, as well as the limited power of using discrete observation distributions [7]. Conclusion and Future Work \u00b6 The scale of the work described here is admittedly limited in terms of the labelling itself still being preliminary and the analysis done on segments extracted from a small, continuous 4.5 hour recording. With the results being obtained, however, we see potential in extending and refining the methods. Our future efforts include: Larger scale model training and tuning Gaining more insights toward a definition of building blocks for charaterization of humpback whale song structure Inclusion of additional features from the signal analysis (e.g., cepstral related coefficients), and use of HMM with continuous observation densities (e.g., Gaussian Mixture Models) Automated segmentation toward an operational use of the classification techiques Acknowledgments \u00b6 The NSF funded installation and maintenance of the MARS cabled observatory through awards 0739828 and 1114794. Hydrophone recording through MARS was supported by the Monterey Bay Aquarium Research Institute, through a grant from the David and Lucile Packard Foundation.","title":"Summary and Conclusions"},{"location":"conclusions/#summary-and-conclusions","text":"Based on classification performance, we empirically determined some potentially good values for signal processing parameters: LPC order: 80-120; Window analysis size: 45-120ms; Window offset: 15ms. The resulting levels of classification accuracy, ranging between 88% and 94%, are encouraging. The relative accuracy of different methods was consistent regardless of parameterization. The VQ classifier allowed us to validate the LPC analysis and clustering phases of our pipeline, while providing a baseline for comparison with the probabilistic models. The Naive Bayes classifier showed great performance despite its simplicity and strong underlying assumption. Although being the least performant, the Markov Chain classifier allowed us to incorporate time dependency as well as a baseline to compare results against other methods, in particular the HMM. The HMM model consistently provided the best performance among the probabilistic models, though just slightly better than Naive Bayes. This is likely due to overfitting, especially given the small size of the dataset, as well as the limited power of using discrete observation distributions [7].","title":"Summary and Conclusions"},{"location":"conclusions/#conclusion-and-future-work","text":"The scale of the work described here is admittedly limited in terms of the labelling itself still being preliminary and the analysis done on segments extracted from a small, continuous 4.5 hour recording. With the results being obtained, however, we see potential in extending and refining the methods. Our future efforts include: Larger scale model training and tuning Gaining more insights toward a definition of building blocks for charaterization of humpback whale song structure Inclusion of additional features from the signal analysis (e.g., cepstral related coefficients), and use of HMM with continuous observation densities (e.g., Gaussian Mixture Models) Automated segmentation toward an operational use of the classification techiques","title":"Conclusion and Future Work"},{"location":"conclusions/#acknowledgments","text":"The NSF funded installation and maintenance of the MARS cabled observatory through awards 0739828 and 1114794. Hydrophone recording through MARS was supported by the Monterey Bay Aquarium Research Institute, through a grant from the David and Lucile Packard Foundation.","title":"Acknowledgments"},{"location":"features/","text":"Feature Extraction \u00b6 Linear Predictive Coding and Vector Quantization \u00b6 We use linear predictive coding (LPC) [2, 3] for feature extraction. The predictive vectors arising from this analysis are then quantized [4] for the statistical modeling. This analysis is applied to consecutive, overlapping time windows where the signal in each window is assumed to be approximately stationary. The LPC analysis on a window \\(x_t\\) results in a vector \\(a_t\\) , which in turn is quantized into a symbol from a codebook of size \\(M\\) . Represented as such symbol sequences, we model the set of song unit instances in a probabilistic manner. The LPC and VQ relevant parameters that we investigate are: \\(P\\) : Order of prediction (length of each vector \\(a_t\\) ) \\(W\\) : Analysis window size (duration of each \\(x_t\\) ) \\(O\\) : Analysis window offset \\(M\\) : Codebook size (number of observable symbols) Codebook Generation and Evaluation \u00b6 To support the vector quantization step, we run a variant of the \\(k\\) -means algorithm for clustering where an LPC based measure of distance (distortion) is used [4]. We generate codebooks of various sizes and evaluate the following metrics: where \\(d(a, b)\\) is the distance or distortion between two LPC vectors, \\(A\\) is the set of training vectors, \\(M\\) is the size of the codebook, and \\(c_m\\) are the centroids of the codebook. The plot in the right-hand side of the figure above -captured from one of the runs we performed- shows the typical behaviour of these metrics as functions of the codebook size \\(M\\) . Quantization example \u00b6 The following is the visualization of a quantization example using a codebook of size \\(M = 512\\) on a ~18s interval: The figure shows: Spectrogram of the selected interval. Corresponding assigned codeword per time window displayed with an arbitrary color mapping. The window offset used was 15ms resulting in about 1200 windows for this interval. (Window size is 45ms.) Qualitatively, although not strictly required, we expect to see similarly colored areas when comparing regions with similar acoustic content, as seems to be well illustrated in the example above. (In general, this does not have to be the case as there is no predetermined ordering of the codewords in the associated codebook.)","title":"Feature Extraction"},{"location":"features/#feature-extraction","text":"","title":"Feature Extraction"},{"location":"features/#linear-predictive-coding-and-vector-quantization","text":"We use linear predictive coding (LPC) [2, 3] for feature extraction. The predictive vectors arising from this analysis are then quantized [4] for the statistical modeling. This analysis is applied to consecutive, overlapping time windows where the signal in each window is assumed to be approximately stationary. The LPC analysis on a window \\(x_t\\) results in a vector \\(a_t\\) , which in turn is quantized into a symbol from a codebook of size \\(M\\) . Represented as such symbol sequences, we model the set of song unit instances in a probabilistic manner. The LPC and VQ relevant parameters that we investigate are: \\(P\\) : Order of prediction (length of each vector \\(a_t\\) ) \\(W\\) : Analysis window size (duration of each \\(x_t\\) ) \\(O\\) : Analysis window offset \\(M\\) : Codebook size (number of observable symbols)","title":"Linear Predictive Coding and Vector Quantization"},{"location":"features/#codebook-generation-and-evaluation","text":"To support the vector quantization step, we run a variant of the \\(k\\) -means algorithm for clustering where an LPC based measure of distance (distortion) is used [4]. We generate codebooks of various sizes and evaluate the following metrics: where \\(d(a, b)\\) is the distance or distortion between two LPC vectors, \\(A\\) is the set of training vectors, \\(M\\) is the size of the codebook, and \\(c_m\\) are the centroids of the codebook. The plot in the right-hand side of the figure above -captured from one of the runs we performed- shows the typical behaviour of these metrics as functions of the codebook size \\(M\\) .","title":"Codebook Generation and Evaluation"},{"location":"features/#quantization-example","text":"The following is the visualization of a quantization example using a codebook of size \\(M = 512\\) on a ~18s interval: The figure shows: Spectrogram of the selected interval. Corresponding assigned codeword per time window displayed with an arbitrary color mapping. The window offset used was 15ms resulting in about 1200 windows for this interval. (Window size is 45ms.) Qualitatively, although not strictly required, we expect to see similarly colored areas when comparing regions with similar acoustic content, as seems to be well illustrated in the example above. (In general, this does not have to be the case as there is no predetermined ordering of the codewords in the associated codebook.)","title":"Quantization example"},{"location":"methods/","text":"Methods and Dataset \u00b6 Methods \u00b6 Feature Extraction: Linear Predictive Coding Clustering and Vector Quantization Classification: VQ Distortion Naive Bayes Markov Chain Model Discrete Hidden Markov Model Dataset \u00b6 Sound recordings acquired through the Monterey Accelerated Research System (MARS) cabled observatory [1] A 4.5-hour continuous and clear recording of one song session from 21 December 2016 One singing whale, variable noise background due to changes in wind speed 256 kHz recording decimated to 16KHz sampling rate 5470 song units manually identified, segmented, and labelled in 22 different classes Constrained to classes having at least 200 instances This resulted in a total of 4539 instances across eight classes as follows: A : 512 Bm : 608 C : 550 E : 713 F : 340 G2 : 307 I3 : 324 II : 1185 Each class instance set was randomly partitioned into: 50% for training 50% for testing Some prototypical song units \u00b6 A DFT of window size 1024 and offset 32 was used in each of the spectrograms below. Time axis is in seconds. An \"A\" instance: \u00b6 An \"F\" instance: \u00b6 A \"G2\" instance: \u00b6","title":"Methods"},{"location":"methods/#methods-and-dataset","text":"","title":"Methods and Dataset"},{"location":"methods/#methods","text":"Feature Extraction: Linear Predictive Coding Clustering and Vector Quantization Classification: VQ Distortion Naive Bayes Markov Chain Model Discrete Hidden Markov Model","title":"Methods"},{"location":"methods/#dataset","text":"Sound recordings acquired through the Monterey Accelerated Research System (MARS) cabled observatory [1] A 4.5-hour continuous and clear recording of one song session from 21 December 2016 One singing whale, variable noise background due to changes in wind speed 256 kHz recording decimated to 16KHz sampling rate 5470 song units manually identified, segmented, and labelled in 22 different classes Constrained to classes having at least 200 instances This resulted in a total of 4539 instances across eight classes as follows: A : 512 Bm : 608 C : 550 E : 713 F : 340 G2 : 307 I3 : 324 II : 1185 Each class instance set was randomly partitioned into: 50% for training 50% for testing","title":"Dataset"},{"location":"methods/#some-prototypical-song-units","text":"A DFT of window size 1024 and offset 32 was used in each of the spectrograms below. Time axis is in seconds.","title":"Some prototypical song units"},{"location":"methods/#an-a-instance","text":"","title":"An \"A\" instance:"},{"location":"methods/#an-f-instance","text":"","title":"An \"F\" instance:"},{"location":"methods/#a-g2-instance","text":"","title":"A \"G2\" instance:"},{"location":"overview/","text":"Overview \u00b6 This poster summarizes ongoing work within the Soundscape group at MBARI toward characterizing humpback whale songs. We use some of the techiques widely applied for human speech analysis, in particular Linear Predictive Coding for signal processing and feature extraction, and Hidden Markov Modeling among other probabilistic models for supervised learning. Although we discuss the relative advantages of these methods in terms of classification accuracy, our main focus is on investigating the effect of some of the involved parameters in the processing pipeline, including order of prediction, analysis window size, and vector quantization codebook size. Overall, we are building a foundation for song structure analysis, toward understanding song as an aspect of humpback whale culture. Goals \u00b6 Build upon signal processing and machine learning techniques and extend them into humpback whale song analysis Evaluate the effect of various signal processing parameters for song unit identification","title":"Overview"},{"location":"overview/#overview","text":"This poster summarizes ongoing work within the Soundscape group at MBARI toward characterizing humpback whale songs. We use some of the techiques widely applied for human speech analysis, in particular Linear Predictive Coding for signal processing and feature extraction, and Hidden Markov Modeling among other probabilistic models for supervised learning. Although we discuss the relative advantages of these methods in terms of classification accuracy, our main focus is on investigating the effect of some of the involved parameters in the processing pipeline, including order of prediction, analysis window size, and vector quantization codebook size. Overall, we are building a foundation for song structure analysis, toward understanding song as an aspect of humpback whale culture.","title":"Overview"},{"location":"overview/#goals","text":"Build upon signal processing and machine learning techniques and extend them into humpback whale song analysis Evaluate the effect of various signal processing parameters for song unit identification","title":"Goals"},{"location":"references/","text":"References \u00b6 [1] Ryan, J., D. Cline, C. Dawe, P. McGill, Y. Zhang, J. Joseph, T. Margolina, M. Caillat, M. Fischer, A. DeVogelaere, A. Stimpert, B. Southall (2016). New passive acoustic monitoring in Monterey Bay National Marine Sanctuary: exploring natural and anthropogenic sounds in a deep soundscape. OCEANS 2016 MTS/IEEE, Monterey, CA USA, doi: 10.1109/OCEANS.2016.7761363 [2] J. Makhoul: Linear prediction: a tutorial review, Proc. IEEE 63, 561-580 (1975) [3] T. Parsons. Voice and speech processing. McGraw-Hill, 1987. [4] B-H. Juang, D.Y. Wong, and A.H. Gray. Distortion performance of vector quantization for LPC voice coding. IEEE Trans. ASSP, 30(2), 1982. [5] T. Mitchell. Machine Learning. McGraw-Hill, 1997. [6] Duda, R, Hart, P, Stork, D. Pattern classification. 2nd Ed. New York: Wiley Interscience, 2000. [7] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257-286, 1989. [8] L. R. Rabiner, S. E. Levinson, and M. M. Sondhi. On the application of vector quantization and hidden Markov models to speaker-independent, isolated word recognition, 1983. [9] Young S. (2008) HMMs and Related Speech Recognition Technologies. In: Benesty J., Sondhi M.M., Huang Y.A. (eds) Springer Handbook of Speech Processing. Springer Handbooks. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-49127-9_27","title":"References"},{"location":"references/#references","text":"[1] Ryan, J., D. Cline, C. Dawe, P. McGill, Y. Zhang, J. Joseph, T. Margolina, M. Caillat, M. Fischer, A. DeVogelaere, A. Stimpert, B. Southall (2016). New passive acoustic monitoring in Monterey Bay National Marine Sanctuary: exploring natural and anthropogenic sounds in a deep soundscape. OCEANS 2016 MTS/IEEE, Monterey, CA USA, doi: 10.1109/OCEANS.2016.7761363 [2] J. Makhoul: Linear prediction: a tutorial review, Proc. IEEE 63, 561-580 (1975) [3] T. Parsons. Voice and speech processing. McGraw-Hill, 1987. [4] B-H. Juang, D.Y. Wong, and A.H. Gray. Distortion performance of vector quantization for LPC voice coding. IEEE Trans. ASSP, 30(2), 1982. [5] T. Mitchell. Machine Learning. McGraw-Hill, 1997. [6] Duda, R, Hart, P, Stork, D. Pattern classification. 2nd Ed. New York: Wiley Interscience, 2000. [7] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257-286, 1989. [8] L. R. Rabiner, S. E. Levinson, and M. M. Sondhi. On the application of vector quantization and hidden Markov models to speaker-independent, isolated word recognition, 1983. [9] Young S. (2008) HMMs and Related Speech Recognition Technologies. In: Benesty J., Sondhi M.M., Huang Y.A. (eds) Springer Handbook of Speech Processing. Springer Handbooks. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-49127-9_27","title":"References"},{"location":"results/","text":"Results \u00b6 VQ Based Classification \u00b6 We performed classification exercises based on the vector quantization distortion measure itself while also investigating different values of the prediction order, \\(P\\) , and the codebook size, \\(M\\) . This initial setup with VQ provided a general baseline for the use and evaluation of the other methods. In human speech analysis, \\(P = 4 + f_s / 1000\\) (where \\(f_s\\) is the sampling frequency) is a typical rule of thumb to estimate an order of prediction that provides a good representation of the overall spectral shape of the signal. With \\(f_s\\) = 16KHz in our case, we used \\(P = 20\\) as a starting value to evaluate the influence of \\(P\\) on classification accuracy. These results indicate an overall accuracy improvement with larger values of both \\(P\\) and \\(M\\) . Zooming into the area of the best parameter combinations ( \\(P \\ge 42\\) , \\(M \\ge 256\\) ): we can see that classification accuracy starts exceeding 92% from \\(P = 60\\) for some of the codebook sizes, then peaking at around \\(P = 100\\) to \\(120\\) , from which point some level of overfitting can be observed for the various codebooks, more prominenty with \\(M = 4096\\) . The maximum accuracy obtained here is 94.1% with \\(P = 100\\) and \\(M = 512\\) . Naive Bayes \u00b6 Here again we evaluated the method with various values of \\(P\\) and \\(M\\) : In this case, larger values of \\(M\\) tend to result in better accuracy, but the difference is in general not very significant among the largest codebooks, \\(M \\ge 1024\\) . The maximum accuracy obtained here is 92.4% with \\(P = 120\\) and \\(M = 4096\\) , with \\(M = 2048\\) very close (92.3%). Markov Chain \u00b6 The resulting accuracy of Markov Chain as a function of \\(P\\) and \\(M\\) is as follows: The overall degraded performance with respect to Naive Bayes can be explained by insufficient data for a more proper training given the significantly greater complexity of the MM model (where the number of parameters to be estimated is cuadratic in the number of symbols, compared to linear in the NB case). The best accuracy 88.4% is obtained with \\(P = 80\\) and \\(M = 1024\\) . Hidden Markov Modeling \u00b6 We evaluated HMM with different values for the various parameters including \\(P\\) , \\(N\\) , and \\(M\\) . An inspection focused on \\(P\\) and \\(M\\) is as follows: Classification accuracy reaches a plateau from \\(P = 80\\) for \\(M \\ge 1024\\) . The maximum accuracy obtained is 92.88% with \\(P = 120\\) and \\(M = 4096\\) . With \\(P = 120\\) , the following parallel coordinates plot shows the effect of \\(N\\) (number of states), \\(M\\) (number of symbols) and \\(I\\) (number of Baum-Welch refinement iterations): In general, we observe that accuracy is adversely impacted with greater \\(I\\) values, which suggests that the initial phase in the HMM training may be enough to provide most of the discrimination power on our small dataset, i.e., the extra refinement iterations tend to make the model incur overfitting. With \\(I = 1\\) from the above plot, the following visualizes the results in more detail: In general, we can see that accuracy increases with larger codebooks. The number of states, however, does not appear to have a clear effect in general. This suggests that the given training data is insufficient to take better advantage of HMM. The maximum accuracy is ~92.8% with \\(M = 4096\\) for various values of \\(N\\) . As an example, with \\(N = 8\\) and \\(M = 4096\\) , here is the resulting confusion matrix with the 8 HMM models over the 2271 test instances: Accuracy as function of W and O \u00b6 We also investigated the effect of the analysis window size \\(W\\) and offset \\(O\\) used for the LPC analysis. For this evaluation we selected a few \\((W, O)\\) combinations and used \\(P = 120\\) and the Naive Bayes classifier. (Note: In this particular evaluation, we used a partition of the dataset into 80% for training and 20% for testing. Although the absolute accuracy values here are not directly comparable with the other evaluations, in this case we are mainly interested in the relative effect across the different \\(W\\) and \\(O\\) values.) This result shows a consistent effect of the offset parameter across the various window sizes, with significant adverse impact on accuracy with larger values and with \\(O = 15\\) ms providing the best performance in these tests. In terms of window size we note an increase in accuracy with increasing \\(W\\) value up to 120 ms but then reduced with \\(W = 150\\) ms. Summary of Accuracy per Method \u00b6 As a function of \\(P\\) : In terms of accuracy error, the overall method comparison is as follows:","title":"Results"},{"location":"results/#results","text":"","title":"Results"},{"location":"results/#vq-based-classification","text":"We performed classification exercises based on the vector quantization distortion measure itself while also investigating different values of the prediction order, \\(P\\) , and the codebook size, \\(M\\) . This initial setup with VQ provided a general baseline for the use and evaluation of the other methods. In human speech analysis, \\(P = 4 + f_s / 1000\\) (where \\(f_s\\) is the sampling frequency) is a typical rule of thumb to estimate an order of prediction that provides a good representation of the overall spectral shape of the signal. With \\(f_s\\) = 16KHz in our case, we used \\(P = 20\\) as a starting value to evaluate the influence of \\(P\\) on classification accuracy. These results indicate an overall accuracy improvement with larger values of both \\(P\\) and \\(M\\) . Zooming into the area of the best parameter combinations ( \\(P \\ge 42\\) , \\(M \\ge 256\\) ): we can see that classification accuracy starts exceeding 92% from \\(P = 60\\) for some of the codebook sizes, then peaking at around \\(P = 100\\) to \\(120\\) , from which point some level of overfitting can be observed for the various codebooks, more prominenty with \\(M = 4096\\) . The maximum accuracy obtained here is 94.1% with \\(P = 100\\) and \\(M = 512\\) .","title":"VQ Based Classification"},{"location":"results/#naive-bayes","text":"Here again we evaluated the method with various values of \\(P\\) and \\(M\\) : In this case, larger values of \\(M\\) tend to result in better accuracy, but the difference is in general not very significant among the largest codebooks, \\(M \\ge 1024\\) . The maximum accuracy obtained here is 92.4% with \\(P = 120\\) and \\(M = 4096\\) , with \\(M = 2048\\) very close (92.3%).","title":"Naive Bayes"},{"location":"results/#markov-chain","text":"The resulting accuracy of Markov Chain as a function of \\(P\\) and \\(M\\) is as follows: The overall degraded performance with respect to Naive Bayes can be explained by insufficient data for a more proper training given the significantly greater complexity of the MM model (where the number of parameters to be estimated is cuadratic in the number of symbols, compared to linear in the NB case). The best accuracy 88.4% is obtained with \\(P = 80\\) and \\(M = 1024\\) .","title":"Markov Chain"},{"location":"results/#hidden-markov-modeling","text":"We evaluated HMM with different values for the various parameters including \\(P\\) , \\(N\\) , and \\(M\\) . An inspection focused on \\(P\\) and \\(M\\) is as follows: Classification accuracy reaches a plateau from \\(P = 80\\) for \\(M \\ge 1024\\) . The maximum accuracy obtained is 92.88% with \\(P = 120\\) and \\(M = 4096\\) . With \\(P = 120\\) , the following parallel coordinates plot shows the effect of \\(N\\) (number of states), \\(M\\) (number of symbols) and \\(I\\) (number of Baum-Welch refinement iterations): In general, we observe that accuracy is adversely impacted with greater \\(I\\) values, which suggests that the initial phase in the HMM training may be enough to provide most of the discrimination power on our small dataset, i.e., the extra refinement iterations tend to make the model incur overfitting. With \\(I = 1\\) from the above plot, the following visualizes the results in more detail: In general, we can see that accuracy increases with larger codebooks. The number of states, however, does not appear to have a clear effect in general. This suggests that the given training data is insufficient to take better advantage of HMM. The maximum accuracy is ~92.8% with \\(M = 4096\\) for various values of \\(N\\) . As an example, with \\(N = 8\\) and \\(M = 4096\\) , here is the resulting confusion matrix with the 8 HMM models over the 2271 test instances:","title":"Hidden Markov Modeling"},{"location":"results/#accuracy-as-function-of-w-and-o","text":"We also investigated the effect of the analysis window size \\(W\\) and offset \\(O\\) used for the LPC analysis. For this evaluation we selected a few \\((W, O)\\) combinations and used \\(P = 120\\) and the Naive Bayes classifier. (Note: In this particular evaluation, we used a partition of the dataset into 80% for training and 20% for testing. Although the absolute accuracy values here are not directly comparable with the other evaluations, in this case we are mainly interested in the relative effect across the different \\(W\\) and \\(O\\) values.) This result shows a consistent effect of the offset parameter across the various window sizes, with significant adverse impact on accuracy with larger values and with \\(O = 15\\) ms providing the best performance in these tests. In terms of window size we note an increase in accuracy with increasing \\(W\\) value up to 120 ms but then reduced with \\(W = 150\\) ms.","title":"Accuracy as function of W and O"},{"location":"results/#summary-of-accuracy-per-method","text":"As a function of \\(P\\) : In terms of accuracy error, the overall method comparison is as follows:","title":"Summary of Accuracy per Method"}]}